{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import random\n",
    "from itertools import product\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class GridWorld():\n",
    "    \"\"\"The size of the grid is (5 x 7)\"\"\"\n",
    "    \"\"\"0 0 1 0 0 0 0\"\"\"\n",
    "    \"\"\"0 0 1 0 0 0 0\"\"\"\n",
    "    \"\"\"S 0 1 0 1 0 0\"\"\"\n",
    "    \"\"\"0 0 0 0 1 0 0\"\"\"\n",
    "    \"\"\"0 0 0 0 1 0 E\"\"\"\n",
    "    def __init__(self):\n",
    "        self.x = 2\n",
    "        self.y = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        if type(action) != int:\n",
    "            raise(TypeError)\n",
    "        \n",
    "        if action == 0:\n",
    "            self.move_left()\n",
    "        elif action == 1:\n",
    "            self.move_right()\n",
    "        elif action == 2:\n",
    "            self.move_up()\n",
    "        elif action == 3:\n",
    "            self.move_down()\n",
    "        else:\n",
    "            raise(ValueError)\n",
    "        \n",
    "        reward = -1\n",
    "        done = self.is_done()\n",
    "        return (self.x, self.y), reward, done\n",
    "    \n",
    "    def move_left(self):\n",
    "        if self.y == 0:\n",
    "            pass\n",
    "        elif self.y == 3 and self.x in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.y == 5 and self.x in [2, 3, 4]:\n",
    "            pass\n",
    "        else:\n",
    "            self.y -= 1\n",
    "            \n",
    "    def move_right(self):\n",
    "        if self.y == 1 and self.x in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.y == 3 and self.x in [2, 3, 4]:\n",
    "            pass\n",
    "        elif self.y == 6:\n",
    "            pass\n",
    "        else:\n",
    "            self.y += 1\n",
    "            \n",
    "    def move_up(self):\n",
    "        if self.x == 0:\n",
    "            pass\n",
    "        elif self.x == 3 and self.y == 2:\n",
    "            pass\n",
    "        else:\n",
    "            self.x -= 1\n",
    "    \n",
    "    def move_down(self):\n",
    "        if self.x == 4:\n",
    "            pass\n",
    "        elif self.x == 1 and self.y == 4:\n",
    "            pass\n",
    "        else:\n",
    "            self.x += 1\n",
    "            \n",
    "    def is_done(self):\n",
    "        if self.x == 4 and self.y == 6:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.x = 2\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "class QAgent():\n",
    "    def __init__(self, update=\"Q-learning\"):\n",
    "        if update not in [\"Monte-Carlo\", \"SARSA\", \"Q-learning\"]:\n",
    "            raise(ValueError)\n",
    "        self.update = update\n",
    "        self.q_table = np.zeros((5, 7, 4))\n",
    "        self.epsilon = 0.9\n",
    "        if update == \"Monte-Carlo\":\n",
    "            self.alpha = 0.01\n",
    "        else:\n",
    "            self.alpha = 0.1\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        x, y = state\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.randint(0, 3)\n",
    "        else:\n",
    "            action_val = self.q_table[x, y, :]\n",
    "            action = int(np.argmax(action_val))\n",
    "        return action\n",
    "    \n",
    "    def update_table(self, update_info):\n",
    "        if self.update == \"Monte-Carlo\":\n",
    "            history = update_info\n",
    "            cum_reward = 0\n",
    "            for transition in history[::-1]:\n",
    "                state, action, reward, next_state = transition\n",
    "                x, y = state\n",
    "                self.q_table[x, y, action] += self.alpha*(cum_reward - self.q_table[x, y, action])\n",
    "                cum_reward += reward\n",
    "                \n",
    "        elif self.update == \"SARSA\":\n",
    "            transition = update_info\n",
    "            state, action, reward, next_state = transition\n",
    "            x, y = state\n",
    "            next_x, next_y = next_state\n",
    "            next_action = self.select_action(next_state)\n",
    "            self.q_table[x, y, action] += self.alpha*(reward + self.q_table[next_x, next_y, next_action] - self.q_table[x, y, action])\n",
    "            \n",
    "        elif self.update == \"Q-learning\":\n",
    "            transition = update_info\n",
    "            state, action, reward, next_state = transition\n",
    "            x, y = state\n",
    "            next_x, next_y = next_state\n",
    "            self.q_table[x, y, action] += self.alpha*(reward + np.amax(self.q_table[next_x, next_y, :]) - self.q_table[x, y, action])\n",
    "        \n",
    "        else:\n",
    "            raise(ValueError)\n",
    "    \n",
    "    def anneal_epsilon(self):\n",
    "        if self.update in [\"Monte-Carlo\", \"SARSA\"]:\n",
    "            self.epsilon -= 0.03\n",
    "            self.epsilon = max(self.epsilon, 0.1)\n",
    "        else:       # Q-learning\n",
    "            self.epsilon -= 0.01\n",
    "            self.epsilon = max(self.epsilon, 0.2)\n",
    "        \n",
    "    def show_table(self):\n",
    "        data = np.argmax(self.q_table, axis=2).tolist()\n",
    "        action_dict = {0: \"<\", 1: \">\", 2: \"^\", 3: \"v\"}\n",
    "        for row, column in product(range(5), range(7)):\n",
    "            if row == 4 and column == 6:\n",
    "                data[row][column] = \"E\"\n",
    "            elif row in [0, 1, 2] and column == 2:\n",
    "                data[row][column] = \"x\"\n",
    "            elif row in [2, 3, 4] and column == 4:\n",
    "                data[row][column] = \"x\"\n",
    "            else:\n",
    "                data[row][column] = action_dict[data[row][column]]\n",
    "        \n",
    "        for row, values in enumerate(data):\n",
    "            if row == 2:\n",
    "                print(f\"start -> {values}\")\n",
    "            else:\n",
    "                print(f\"         {values}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Monte-Carlo\n",
    "env = GridWorld()\n",
    "agent = QAgent(update=\"Monte-Carlo\")\n",
    "\n",
    "for n_epi in range(10000):\n",
    "    done = False\n",
    "    history = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        history.append((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "    agent.update_table(history)\n",
    "    agent.anneal_epsilon()\n",
    "\n",
    "agent.show_table()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         ['v', 'v', 'x', 'v', '<', '<', 'v']\n",
      "         ['v', '<', 'x', '>', '>', 'v', 'v']\n",
      "start -> ['v', 'v', 'x', '^', 'x', 'v', 'v']\n",
      "         ['>', '>', '>', '^', 'x', '>', 'v']\n",
      "         ['>', '^', '>', '^', 'x', '>', 'E']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# SARSA\n",
    "env = GridWorld()\n",
    "agent = QAgent(update=\"SARSA\")\n",
    "\n",
    "for n_epi in range(1000):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.update_table((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "    agent.anneal_epsilon()\n",
    "\n",
    "agent.show_table() "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         ['^', '<', 'x', '>', '^', '>', 'v']\n",
      "         ['^', '<', 'x', '>', '>', '>', 'v']\n",
      "start -> ['>', 'v', 'x', '^', 'x', '>', 'v']\n",
      "         ['>', '>', '>', '^', 'x', '>', 'v']\n",
      "         ['v', '>', 'v', '^', 'x', '>', 'E']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Q-learning\n",
    "env = GridWorld()\n",
    "agent = QAgent(update=\"Q-learning\")\n",
    "\n",
    "for n_epi in range(1000):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.update_table((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "    agent.anneal_epsilon()\n",
    "\n",
    "agent.show_table() "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         ['>', 'v', 'x', 'v', '>', '>', 'v']\n",
      "         ['v', 'v', 'x', '>', '>', '>', 'v']\n",
      "start -> ['>', 'v', 'x', '^', 'x', 'v', 'v']\n",
      "         ['>', '>', '>', '^', 'x', 'v', 'v']\n",
      "         ['<', '^', '>', '^', 'x', '>', 'E']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "86ccdcb783a3baf0aadc75153d57a6ddbd2dc9e8dd9d72826f65f1dd39ff8e8a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}