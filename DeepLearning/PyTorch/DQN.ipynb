{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd04bdc6f543d21430f19fdc68f48a348338922fbb620c2c2e274fc8ce374f8d71d",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import pybithumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPISOPES = 1000\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "GAMMA = 0.9\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "connect_key = \"d5c7f4458a58322ac7573f9f8193d4f2\"\n",
    "secret_key = \"aacd7c9c31a4bbcf30d5088a1b22e338\"\n",
    "bithumb = pybithumb.Bithumb(connect_key, secret_key)\n",
    "sample = bithumb.get_candlestick(\"ETH\", \"KRW\", \"30m\")\n",
    "\n",
    "cut = int(len(sample.index)*0.7)\n",
    "train_sample = sample.iloc[:cut, :]\n",
    "test_sample = sample.iloc[cut:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeBook:\n",
    "    def __init__(self, sample):\n",
    "        self.acc_rtn = 1.\n",
    "        self.bought = False\n",
    "        self.block = 0\n",
    "        self.sample = self.preprosess(sample)\n",
    "\n",
    "    def preprosess(self, sample):\n",
    "        sample = sample[['open', 'high', 'low', 'close', 'volume']]\n",
    "        sample['rtn'] = sample['open']/sample.shift(1)['open']\n",
    "        sample['open'] = sample['open'].pct_change()\n",
    "        sample['high'] = sample['high'].pct_change().shift(1)\n",
    "        sample['low'] = sample['low'].pct_change().shift(1)\n",
    "        sample['close'] = sample['close'].pct_change().shift(1)\n",
    "        sample['volume'] = sample['volume'].pct_change().shift(1)\n",
    "        sample = sample.dropna()\n",
    "        #sample = sample.iloc[-48*7:, :]\n",
    "        return sample.to_numpy()\n",
    "\n",
    "    def step(self, action):\n",
    "        actions = ['buy', 'sell']\n",
    "        action = actions[action]\n",
    "\n",
    "        # update current status\n",
    "        if self.bought and action == \"sell\":\n",
    "            self.bought = False\n",
    "        elif not self.bought and action == \"buy\":\n",
    "            self.bought = True\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # get reward\n",
    "        if self.bought and action == \"buy\":\n",
    "            reward = self.sample[self.block][5]\n",
    "        elif self.bought and action == \"sell\":\n",
    "            reward = self.sample[self.block][5] - 0.004\n",
    "        elif not self.bought and action == \"buy\":\n",
    "            reward = 1. - 0.004\n",
    "        else:\n",
    "            reward = 1.\n",
    "        \n",
    "        self.acc_rtn *= reward\n",
    "\n",
    "        self.block += 1\n",
    "\n",
    "        done = self.is_done()\n",
    "        return self.sample[self.block-1][:5], reward, done\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.acc_rtn <= 0.8:\n",
    "            return True\n",
    "        elif self.block == len(self.sample):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.acc_rtn = 1.\n",
    "        self.bought = False\n",
    "        self.block = 0\n",
    "        return self.sample[self.block][:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(5, 64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
    "        self.steps_done = 0\n",
    "        self.memory = deque(maxlen=100)\n",
    "    \n",
    "    def memorize(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, torch.FloatTensor([reward]), torch.FloatTensor([next_state])))\n",
    "    \n",
    "    def act(self, state):\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.*self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if random.random() > eps_threshold:\n",
    "            return self.model(state).data.max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.LongTensor([[random.randrange(2)]])\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        \n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "\n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
    "\n",
    "        expected_q = rewards + (GAMMA*max_next_q)\n",
    "\n",
    "        #loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        loss = F.smooth_l1_loss(current_q.squeeze(), expected_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPISODE: 1\t ACCUMULATED RETURNS: 1.217876985407098\n",
      "EPISODE: 2\t ACCUMULATED RETURNS: 1.0984566346543\n",
      "EPISODE: 3\t ACCUMULATED RETURNS: 1.3189032445670528\n",
      "EPISODE: 4\t ACCUMULATED RETURNS: 1.107893576079098\n",
      "EPISODE: 5\t ACCUMULATED RETURNS: 1.2711624142638478\n",
      "EPISODE: 6\t ACCUMULATED RETURNS: 1.1385980946256924\n",
      "EPISODE: 7\t ACCUMULATED RETURNS: 1.2680479011949355\n",
      "EPISODE: 8\t ACCUMULATED RETURNS: 1.2265661743511442\n",
      "EPISODE: 9\t ACCUMULATED RETURNS: 1.2550331054100727\n",
      "EPISODE: 10\t ACCUMULATED RETURNS: 1.1552487106217868\n",
      "EPISODE: 11\t ACCUMULATED RETURNS: 1.2334430880766656\n",
      "EPISODE: 12\t ACCUMULATED RETURNS: 1.8261765402533088\n",
      "EPISODE: 13\t ACCUMULATED RETURNS: 1.1222763271256766\n",
      "EPISODE: 14\t ACCUMULATED RETURNS: 1.2458554328752958\n",
      "EPISODE: 15\t ACCUMULATED RETURNS: 1.3576112164859868\n",
      "EPISODE: 16\t ACCUMULATED RETURNS: 1.0990983925661508\n",
      "EPISODE: 17\t ACCUMULATED RETURNS: 1.245014898069071\n",
      "EPISODE: 18\t ACCUMULATED RETURNS: 1.4058473066598434\n",
      "EPISODE: 19\t ACCUMULATED RETURNS: 1.2821855468418673\n",
      "EPISODE: 20\t ACCUMULATED RETURNS: 1.3537432473200603\n",
      "EPISODE: 21\t ACCUMULATED RETURNS: 1.1966887474580832\n",
      "EPISODE: 22\t ACCUMULATED RETURNS: 1.2282161460593697\n",
      "EPISODE: 23\t ACCUMULATED RETURNS: 1.066383822024448\n",
      "EPISODE: 24\t ACCUMULATED RETURNS: 1.0167820613638652\n",
      "EPISODE: 25\t ACCUMULATED RETURNS: 1.358778706563687\n",
      "EPISODE: 26\t ACCUMULATED RETURNS: 1.4590768618322794\n",
      "EPISODE: 27\t ACCUMULATED RETURNS: 1.567423272880451\n",
      "EPISODE: 28\t ACCUMULATED RETURNS: 1.3826168924113647\n",
      "EPISODE: 29\t ACCUMULATED RETURNS: 1.289649664914968\n",
      "EPISODE: 30\t ACCUMULATED RETURNS: 1.546670823729796\n",
      "EPISODE: 31\t ACCUMULATED RETURNS: 1.508095732435901\n",
      "EPISODE: 32\t ACCUMULATED RETURNS: 1.3305845708216446\n",
      "EPISODE: 33\t ACCUMULATED RETURNS: 1.2758659589231927\n",
      "EPISODE: 34\t ACCUMULATED RETURNS: 1.124030303387956\n",
      "EPISODE: 35\t ACCUMULATED RETURNS: 1.3037510551180633\n",
      "EPISODE: 36\t ACCUMULATED RETURNS: 1.303787276592431\n",
      "EPISODE: 37\t ACCUMULATED RETURNS: 1.0456061229885762\n",
      "EPISODE: 38\t ACCUMULATED RETURNS: 1.4818309489518469\n",
      "EPISODE: 39\t ACCUMULATED RETURNS: 1.5082567888640186\n",
      "EPISODE: 40\t ACCUMULATED RETURNS: 1.3366583919994388\n",
      "EPISODE: 41\t ACCUMULATED RETURNS: 1.5833540327073772\n",
      "EPISODE: 42\t ACCUMULATED RETURNS: 1.3537384418769716\n",
      "EPISODE: 43\t ACCUMULATED RETURNS: 1.6185502139658052\n",
      "EPISODE: 44\t ACCUMULATED RETURNS: 1.2753421247433847\n",
      "EPISODE: 45\t ACCUMULATED RETURNS: 1.3386033686357302\n",
      "EPISODE: 46\t ACCUMULATED RETURNS: 1.536491767749327\n",
      "EPISODE: 47\t ACCUMULATED RETURNS: 1.193714694477694\n",
      "EPISODE: 48\t ACCUMULATED RETURNS: 1.556910790071287\n",
      "EPISODE: 49\t ACCUMULATED RETURNS: 1.0050989107310098\n",
      "EPISODE: 50\t ACCUMULATED RETURNS: 1.7444027745737745\n",
      "EPISODE: 51\t ACCUMULATED RETURNS: 1.3117239106072829\n",
      "EPISODE: 52\t ACCUMULATED RETURNS: 1.1890211683784095\n",
      "EPISODE: 53\t ACCUMULATED RETURNS: 1.2388812209015174\n",
      "EPISODE: 54\t ACCUMULATED RETURNS: 1.4028971849943308\n",
      "EPISODE: 55\t ACCUMULATED RETURNS: 1.592880918453908\n",
      "EPISODE: 56\t ACCUMULATED RETURNS: 1.384684160998196\n",
      "EPISODE: 57\t ACCUMULATED RETURNS: 1.5284534790529924\n",
      "EPISODE: 58\t ACCUMULATED RETURNS: 1.4936110784829046\n",
      "EPISODE: 59\t ACCUMULATED RETURNS: 1.7533317646866708\n",
      "EPISODE: 60\t ACCUMULATED RETURNS: 1.4646446629263703\n",
      "EPISODE: 61\t ACCUMULATED RETURNS: 1.1280400423887738\n",
      "EPISODE: 62\t ACCUMULATED RETURNS: 1.5700147169908676\n",
      "EPISODE: 63\t ACCUMULATED RETURNS: 1.3867335853452407\n",
      "EPISODE: 64\t ACCUMULATED RETURNS: 1.513457928941584\n",
      "EPISODE: 65\t ACCUMULATED RETURNS: 1.471593083770268\n",
      "EPISODE: 66\t ACCUMULATED RETURNS: 1.5369844825569108\n",
      "EPISODE: 67\t ACCUMULATED RETURNS: 1.358359821226552\n",
      "EPISODE: 68\t ACCUMULATED RETURNS: 1.205050719228751\n",
      "EPISODE: 69\t ACCUMULATED RETURNS: 1.3121617788340618\n",
      "EPISODE: 70\t ACCUMULATED RETURNS: 1.2240314240197685\n",
      "EPISODE: 71\t ACCUMULATED RETURNS: 1.2530854321988247\n",
      "EPISODE: 72\t ACCUMULATED RETURNS: 1.482582003582614\n",
      "EPISODE: 73\t ACCUMULATED RETURNS: 1.6903044538289604\n",
      "EPISODE: 74\t ACCUMULATED RETURNS: 1.3611553248722277\n",
      "EPISODE: 75\t ACCUMULATED RETURNS: 1.3433188503982179\n",
      "EPISODE: 76\t ACCUMULATED RETURNS: 1.2182251492956517\n",
      "EPISODE: 77\t ACCUMULATED RETURNS: 1.3759342350846415\n",
      "EPISODE: 78\t ACCUMULATED RETURNS: 1.3002174331383898\n",
      "EPISODE: 79\t ACCUMULATED RETURNS: 1.3656151703650952\n",
      "EPISODE: 80\t ACCUMULATED RETURNS: 1.4093622243666273\n",
      "EPISODE: 81\t ACCUMULATED RETURNS: 1.4056219574503268\n",
      "EPISODE: 82\t ACCUMULATED RETURNS: 1.205557062529041\n",
      "EPISODE: 83\t ACCUMULATED RETURNS: 1.4608188037567522\n",
      "EPISODE: 84\t ACCUMULATED RETURNS: 1.133275131774338\n",
      "EPISODE: 85\t ACCUMULATED RETURNS: 1.6324622775452258\n",
      "EPISODE: 86\t ACCUMULATED RETURNS: 1.2293454216015747\n",
      "EPISODE: 87\t ACCUMULATED RETURNS: 1.4947404245636196\n",
      "EPISODE: 88\t ACCUMULATED RETURNS: 1.5916014601167903\n",
      "EPISODE: 89\t ACCUMULATED RETURNS: 1.1312554481217678\n",
      "EPISODE: 90\t ACCUMULATED RETURNS: 1.5415206732301403\n",
      "EPISODE: 91\t ACCUMULATED RETURNS: 1.1971765620664856\n",
      "EPISODE: 92\t ACCUMULATED RETURNS: 1.6007331586455662\n",
      "EPISODE: 93\t ACCUMULATED RETURNS: 1.1322400526739533\n",
      "EPISODE: 94\t ACCUMULATED RETURNS: 1.5890501471081104\n",
      "EPISODE: 95\t ACCUMULATED RETURNS: 1.3315093478008582\n",
      "EPISODE: 96\t ACCUMULATED RETURNS: 1.3376351468266607\n",
      "EPISODE: 97\t ACCUMULATED RETURNS: 1.504994797331488\n",
      "EPISODE: 98\t ACCUMULATED RETURNS: 1.4607599589041624\n",
      "EPISODE: 99\t ACCUMULATED RETURNS: 1.2893224862583714\n",
      "EPISODE: 100\t ACCUMULATED RETURNS: 1.4108094724993492\n",
      "EPISODE: 101\t ACCUMULATED RETURNS: 1.5869577457996416\n",
      "EPISODE: 102\t ACCUMULATED RETURNS: 1.3332988752196733\n",
      "EPISODE: 103\t ACCUMULATED RETURNS: 1.3891966495415133\n",
      "EPISODE: 104\t ACCUMULATED RETURNS: 1.4088349726175338\n",
      "EPISODE: 105\t ACCUMULATED RETURNS: 1.6463295973806986\n",
      "EPISODE: 106\t ACCUMULATED RETURNS: 1.5243682390788698\n",
      "EPISODE: 107\t ACCUMULATED RETURNS: 1.3982093787266177\n",
      "EPISODE: 108\t ACCUMULATED RETURNS: 1.2716130273510795\n",
      "EPISODE: 109\t ACCUMULATED RETURNS: 1.4799996053551177\n",
      "EPISODE: 110\t ACCUMULATED RETURNS: 1.3899829178613938\n",
      "EPISODE: 111\t ACCUMULATED RETURNS: 1.6094484726461964\n",
      "EPISODE: 112\t ACCUMULATED RETURNS: 1.3016909044498655\n",
      "EPISODE: 113\t ACCUMULATED RETURNS: 1.3117123343642183\n",
      "EPISODE: 114\t ACCUMULATED RETURNS: 1.112022315404341\n",
      "EPISODE: 115\t ACCUMULATED RETURNS: 1.488099659378535\n",
      "EPISODE: 116\t ACCUMULATED RETURNS: 1.2703976612041634\n",
      "EPISODE: 117\t ACCUMULATED RETURNS: 1.409293483846925\n",
      "EPISODE: 118\t ACCUMULATED RETURNS: 1.3962879163995923\n",
      "EPISODE: 119\t ACCUMULATED RETURNS: 1.0795267365293166\n",
      "EPISODE: 120\t ACCUMULATED RETURNS: 1.2474958401042773\n",
      "EPISODE: 121\t ACCUMULATED RETURNS: 1.0843600396586297\n",
      "EPISODE: 122\t ACCUMULATED RETURNS: 1.2515560830894465\n",
      "EPISODE: 123\t ACCUMULATED RETURNS: 1.2895790301448498\n",
      "EPISODE: 124\t ACCUMULATED RETURNS: 1.1033309261682942\n",
      "EPISODE: 125\t ACCUMULATED RETURNS: 1.2196268993957635\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-08593274abf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-86476652878f>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = TradeBook(train_sample)\n",
    "agent = DQNAgent()\n",
    "score_history = []\n",
    "for e in range(1, EPISOPES+1):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        state = torch.FloatTensor([state])\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action.item())\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state)\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1 \n",
    "\n",
    "        if done:\n",
    "            print(f\"EPISODE: {e}\\t ACCUMULATED RETURNS: {env.acc_rtn}\")\n",
    "            score_history.append(env.acc_rtn)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "env = TradeBook(test_sample)\n",
    "state = env.reset()\n",
    "steps = 0\n",
    "\n",
    "while True:\n",
    "    state = torch.FloatTensor([state])\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done = env.step(action.item())\n",
    "\n",
    "    agent.memorize(state, action, reward, next_state)\n",
    "    agent.learn()\n",
    "\n",
    "    state = next_state\n",
    "    steps += 1\n",
    "\n",
    "    if done:\n",
    "        print(f\"ACCUMULATED RETURNS for test: {env.acc_rtn}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_history)\n",
    "plt.ylabel('returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}